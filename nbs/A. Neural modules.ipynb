{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e79ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Iterable, Optional\n",
    "\n",
    "# import xformers.ops as xops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "# Code in this file is mostly borrowed from\n",
    "# https://github.com/openai/whisper/blob/main/whisper/model.py\n",
    "# and is under the MIT License\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "# Used in Î¼P to initialize the weights and configure the optimizer\n",
    "# These two layers map the transformer width into a fixed dimension\n",
    "class LinearHead(nn.Linear):\n",
    "    pass\n",
    "\n",
    "class QueryHead(nn.Linear):\n",
    "    pass\n",
    "\n",
    "# based on https://github.com/karpathy/minGPT/blob/master/mingpt/model.py#L163\n",
    "def init_transformer(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Embedding)):\n",
    "        torch.nn.init.trunc_normal_(m.weight, std=.02)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "        torch.nn.init.constant_(m.weight, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5c1511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sinusoids(length, channels, max_timescale=10000):\n",
    "    \"\"\"Returns sinusoids for positional embedding\"\"\"\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09be4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, qk_scale: float = 1):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.sqrt_qk_scale = math.sqrt(qk_scale)\n",
    "        self.query = QueryHead(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        xa: Optional[Tensor] = None,\n",
    "        causal = False,\n",
    "        kv_cache: Optional[dict] = None,\n",
    "    ):\n",
    "        q = self.query(x)\n",
    "\n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            # hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\n",
    "            # otherwise, perform key/value projections for self- or cross-attention as usual.\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "            # for cross-attention, calculate keys and values once and reuse in subsequent calls.\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        if self.sqrt_qk_scale != 1:\n",
    "            q *= self.sqrt_qk_scale\n",
    "            k *= self.sqrt_qk_scale\n",
    "\n",
    "        wv, qk = self.qkv_attention_pth20(q, k, v, causal)\n",
    "#         wv, qk = self.qkv_attention_xformers(q, k, v, causal)\n",
    "        \n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def qkv_attention_pth20(\n",
    "        self, q: Tensor, k: Tensor, v: Tensor, causal = False\n",
    "    ):\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        # modified for better performance under PyTorch 2.0\n",
    "        wv = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=causal)\n",
    "\n",
    "        # previously we've returned q@k which we don't have now\n",
    "        # since it's not actually used anywhere else, let's just keep two return values for compatibility\n",
    "        return wv.permute(0, 2, 1, 3).flatten(start_dim=2), None\n",
    "\n",
    "    def qkv_attention_xformers(\n",
    "        self, q: Tensor, k: Tensor, v: Tensor, causal = False\n",
    "    ):\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        q = q.view(*q.shape[:2], self.n_head, -1)\n",
    "        k = k.view(*k.shape[:2], self.n_head, -1)\n",
    "        v = v.view(*v.shape[:2], self.n_head, -1)\n",
    "        \n",
    "        bias = xops.LowerTriangularMask() if causal else None\n",
    "        wv = xops.memory_efficient_attention(q,k,v, attn_bias=bias)\n",
    "\n",
    "        # previously we've returned q@k which we don't have now\n",
    "        # since it's not actually used anywhere else, let's just keep two return values for compatibility\n",
    "        return wv.flatten(start_dim=2), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f805216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False,\n",
    "                 qk_scale: float = 1, ffn_mult: int = 4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(n_state, n_head, qk_scale=qk_scale)\n",
    "        self.attn_ln = LayerNorm(n_state)\n",
    "\n",
    "        self.cross_attn = (\n",
    "            MultiHeadAttention(n_state, n_head, qk_scale=qk_scale) if cross_attention else None\n",
    "        )\n",
    "        self.cross_attn_ln = LayerNorm(n_state) if cross_attention else None\n",
    "\n",
    "        n_mlp = n_state * ffn_mult\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_state, n_mlp), nn.GELU(), nn.Linear(n_mlp, n_state)\n",
    "        )\n",
    "        self.mlp_ln = LayerNorm(n_state)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        xa: Optional[Tensor] = None,\n",
    "        causal = False,\n",
    "        kv_cache: Optional[dict] = None,\n",
    "    ):\n",
    "        x = x + self.attn(self.attn_ln(x), causal=causal, kv_cache=kv_cache)[0]\n",
    "        if self.cross_attn:\n",
    "            x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]\n",
    "        x = x + self.mlp(self.mlp_ln(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eddabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, depth=6, width=384, n_head=6, length=1500, codes=1024, qk_scale=1, pos_embs=None):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.embedding = nn.Embedding(codes, width)\n",
    "\n",
    "        if pos_embs is None: pos_embs = sinusoids(length, width)\n",
    "        self.register_buffer(\"positional_embedding\", pos_embs)\n",
    "\n",
    "        self.layers = nn.Sequential(*[\n",
    "            ResidualAttentionBlock(width, n_head, qk_scale=qk_scale) for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        \n",
    "        self.apply(init_transformer)\n",
    "        \n",
    "    def forward(self, Stoks):\n",
    "        xin = self.embedding(Stoks)\n",
    "        \n",
    "        assert xin.shape[1:] == self.positional_embedding.shape, \"incorrect semantic token shape\"\n",
    "        xin = (xin + self.positional_embedding).to(xin.dtype)\n",
    "\n",
    "        return self.ln_post(self.layers(xin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ed5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, depth=6, width=384, n_head=6, length=1500, codes=1024, qk_scale=1, pos_embs=None):\n",
    "        super().__init__()\n",
    "        self.length = length\n",
    "        self.codes = codes\n",
    "    \n",
    "        # embed semantic tokens\n",
    "        self.embedding = nn.Embedding(codes+1, width)\n",
    "        if pos_embs is None: pos_embs = sinusoids(length, width)\n",
    "        self.register_buffer(\"positional_embedding\", pos_embs)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            ResidualAttentionBlock(width, n_head, qk_scale=qk_scale, cross_attention=True) for _ in range(depth)\n",
    "        ])\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        \n",
    "        self.apply(init_transformer)\n",
    "        \n",
    "    def forward(self, Stoks, xenc):\n",
    "        sot = self.embedding(torch.tensor([self.codes]).cuda()).repeat(Stoks.shape[0],1,1)\n",
    "        if Stoks.shape[-1] > 0:\n",
    "            if Stoks.shape[-1] >= self.length:\n",
    "                Stoks = Stoks[:,:-1]\n",
    "            Sembs = self.embedding(Stoks)\n",
    "            Sembs = torch.cat([sot, Sembs], dim=-2)\n",
    "        else:\n",
    "            Sembs = sot\n",
    "\n",
    "        xin = (Sembs + self.positional_embedding[:Sembs.shape[1]]).to(xenc.dtype)\n",
    "    \n",
    "        x = xin\n",
    "        for l in self.layers: x = l(x, xenc, causal=True)\n",
    "        \n",
    "        x = self.ln_post(x)\n",
    "        \n",
    "        logits = (x @ self.embedding.weight.to(x.dtype).T).float()\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d689d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SumDecoder(nn.Module):\n",
    "    def __init__(self, depth=6, width=384, n_head=6, length=9000, codes=1024, qk_scale=1, pos_embs=None):\n",
    "        super().__init__()\n",
    "        self.length = length\n",
    "        self.codes = codes\n",
    "    \n",
    "        # embed semantic tokens\n",
    "        self.embedding = nn.Embedding(codes+1, width)\n",
    "        if pos_embs is None: pos_embs = sinusoids(length, width)\n",
    "        self.register_buffer(\"positional_embedding\", pos_embs)\n",
    "        \n",
    "        # before adding the encoder features\n",
    "        self.layers = nn.ModuleList([\n",
    "            ResidualAttentionBlock(width, n_head, qk_scale=qk_scale) for _ in range(math.floor(depth/2))\n",
    "        ])\n",
    "\n",
    "        # after adding the encoder features\n",
    "        self.layers2 = nn.ModuleList([\n",
    "            ResidualAttentionBlock(width, n_head, qk_scale=qk_scale) for _ in range(math.ceil(depth/2))\n",
    "        ])\n",
    "\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        \n",
    "        self.apply(init_transformer)\n",
    "        \n",
    "    def forward(self, toks, xenc):\n",
    "        sot = self.embedding(torch.tensor([self.codes]).cuda()).repeat(toks.shape[0],1,1)\n",
    "        if toks.shape[-1] > 0:\n",
    "            if toks.shape[-1] >= self.length:\n",
    "                toks = toks[:,:-1]\n",
    "            embs = self.embedding(toks)\n",
    "            embs = torch.cat([sot, embs], dim=-2)\n",
    "        else:\n",
    "            embs = sot\n",
    "\n",
    "        xin = (embs + self.positional_embedding[:embs.shape[1]]).to(xenc.dtype)\n",
    "    \n",
    "        x = xin\n",
    "\n",
    "        for l in self.layers: x = l(x, causal=True)\n",
    "        \n",
    "        x += xenc.repeat_interleave(self.length // xenc.shape[-2], dim=-2)[:,:embs.shape[1]]\n",
    "\n",
    "        for l in self.layers2: x = l(x, causal=True)\n",
    "        \n",
    "        x = self.ln_post(x)\n",
    "        \n",
    "        logits = (x @ self.embedding.weight.to(x.dtype).T).float()\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00406652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa6ef35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
