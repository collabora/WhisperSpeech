{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdbe3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp prepare_t2s_txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf56fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fastprogress import progress_bar\n",
    "from fastcore.script import *\n",
    "\n",
    "import whisper, whisperx\n",
    "from whisperspeech import utils, vad_merge\n",
    "import webdataset as wds\n",
    "\n",
    "from whisperspeech.inference import get_compute_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d80d3b",
   "metadata": {},
   "source": [
    "# T2S dataset preparation\n",
    "\n",
    "We take a webdataset shard and extract semantic tokens and transcriptions from it.\n",
    "\n",
    "We use VAD chunks merged with randomized maximum length to also generate some short samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8bf372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class Transcriber:\n",
    "    \"\"\"\n",
    "    A helper class to transcribe a batch of 30 second audio chunks.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_size, lang=False):\n",
    "        self.model_size = model_size\n",
    "        # try to translate long language names to codes\n",
    "        lang = whisper.tokenizer.TO_LANGUAGE_CODE.get(lang, lang)\n",
    "        self.model = whisperx.asr.load_model(\n",
    "            model_size, get_compute_device(), compute_type=\"float16\", language=lang,\n",
    "            asr_options=dict(repetition_penalty=1, no_repeat_ngram_size=0, prompt_reset_on_temperature=0.5,\n",
    "                             max_new_tokens=500, clip_timestamps=None, hallucination_silence_threshold=None))\n",
    "        # without calling vad_model at least once the rest segfaults for some reason...\n",
    "        self.model.vad_model({\"waveform\": torch.zeros(1, 16000), \"sample_rate\": 16000})\n",
    "        \n",
    "    def transcribe(self, batch):\n",
    "        batch = whisper.log_mel_spectrogram(batch, 128 if self.model_size == 'large-v3' else 80)\n",
    "        embs = self.model.model.encode(batch.cpu().numpy())\n",
    "        return self.model.tokenizer.tokenizer.decode_batch([x.sequences_ids[0] for x in \n",
    "            self.model.model.model.generate(\n",
    "                embs,\n",
    "                [self.model.model.get_prompt(self.model.tokenizer, [], without_timestamps=True)]*len(batch),\n",
    "            )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f271d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@call_parse\n",
    "def prepare_txt(\n",
    "    input:str,           # input shard URL/path\n",
    "    output:str,          # output shard path\n",
    "    n_samples:int=None, # process a limited amount of samples\n",
    "    batch_size:int=16, # process several segments at once\n",
    "    transcription_model:str=\"medium\",\n",
    "    language:str=\"en\",\n",
    "):\n",
    "    transcriber = Transcriber(transcription_model, lang=language)\n",
    "\n",
    "    total = n_samples//batch_size if n_samples else 'noinfer'\n",
    "    if n_samples: print(f\"Benchmarking run of {n_samples} samples ({total} batches)\")\n",
    "\n",
    "    import math, time\n",
    "    start = time.time()\n",
    "    ds = wds.WebDataset([utils.derived_name(input, 'mvad')]).decode()\n",
    "    total = math.ceil(sum([len(x['raw.spk_emb.npy']) for x in ds])/batch_size)\n",
    "    print(f\"Counting {total} batches: {time.time()-start:.2f}\")\n",
    "\n",
    "    ds = vad_merge.chunked_audio_dataset([input], 'raw').compose(\n",
    "        utils.resampler(16000, 'samples_16k'),\n",
    "    )\n",
    "\n",
    "    ds = ds.compose(\n",
    "        wds.to_tuple('__key__', 'rpad', 'samples_16k'),\n",
    "        wds.batched(64),\n",
    "    )\n",
    "\n",
    "    dl = wds.WebLoader(ds, num_workers=1, batch_size=None).unbatched().batched(batch_size)\n",
    "\n",
    "    with utils.AtomicTarWriter(output, throwaway=n_samples is not None) as sink:\n",
    "        for keys, rpads, samples in progress_bar(dl, total=total):\n",
    "            csamples = samples.to(get_compute_device())\n",
    "            txts = transcriber.transcribe(csamples)\n",
    "\n",
    "            for key, rpad, txt in zip(keys, rpads, txts):\n",
    "                sink.write({\n",
    "                    \"__key__\": key,\n",
    "                    \"txt\": txt,\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c35e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../test-dataset/medium-txt/'\n",
    "!mkdir -p {dir}\n",
    "prepare_txt('../test-dataset/audio/test-shard.tar', dir+'test-shard.tar', transcription_model='medium', language='en')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
